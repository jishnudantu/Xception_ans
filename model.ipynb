{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Reshape, Activation, Dropout\n",
    "from keras.layers import LSTM, merge, Dense\n",
    "\n",
    "def VQA_MODEL():\n",
    "    image_feature_size = 2048\n",
    "    word_feature_size = 300\n",
    "    number_of_LSTM = 3\n",
    "    number_of_hidden_units_LSTM = 512\n",
    "    max_length_questions = 30\n",
    "    number_of_dense_layers = 4\n",
    "    number_of_hidden_units = 1024\n",
    "    activation_function = 'tanh'\n",
    "    dropout = 0.5\n",
    "\n",
    "\n",
    "    # Image model\n",
    "    model_image = Sequential()\n",
    "    model_image.add(Reshape((image_feature_size,), input_shape=(image_feature_size,)))\n",
    "\n",
    "    # Language Model\n",
    "    model_language = Sequential()\n",
    "    model_language.add(LSTM(number_of_hidden_units_LSTM, return_sequences=True, input_shape=(max_length_questions, word_feature_size)))\n",
    "    model_language.add(LSTM(number_of_hidden_units_LSTM, return_sequences=True))\n",
    "    model_language.add(LSTM(number_of_hidden_units_LSTM, return_sequences=False))\n",
    "\n",
    "    # combined model\n",
    "    model = Sequential()\n",
    "    model.add(merge.Concatenate([model_language, model_image], axis=1))\n",
    "\n",
    "    for _ in range(number_of_dense_layers):\n",
    "        model.add(Dense(number_of_hidden_units, kernel_initializer='uniform'))\n",
    "        model.add(Activation(activation_function))\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Dense(1000))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import h5py\n",
    "from keras.applications.xception import Xception\n",
    "from keras.applications.xception import preprocess_input\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import load_img\n",
    "import numpy as np\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = h5py.File('/home/ubuntu/data/data_processed/data_prepro.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['ques_train'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "question_data['ques_length_train'][:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "question_data = json.load(open('/home/ubuntu/data/questions_annotations/vqa_raw_train.json','r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MC_ans': ['fish',\n",
       "  'no needs more',\n",
       "  'green',\n",
       "  'special',\n",
       "  'white',\n",
       "  '1',\n",
       "  'red',\n",
       "  'yes',\n",
       "  'not',\n",
       "  'no',\n",
       "  'blue',\n",
       "  '4',\n",
       "  'low in fat',\n",
       "  '8691',\n",
       "  'wool',\n",
       "  '3',\n",
       "  '2',\n",
       "  'east and west'],\n",
       " 'ans': 'yes',\n",
       " 'img_path': 'val2014/COCO_val2014_000000552610.jpg',\n",
       " 'ques_id': 5526102,\n",
       " 'question': 'Is there water in the water bottle?'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_data[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def image_preprocess(img_path):\n",
    "    image = load_img(img_path, target_size=(299, 299))\n",
    "    image = img_to_array(image)\n",
    "\n",
    "    # our input image is now represented as a NumPy array of shape\n",
    "    # (inputShape[0], inputShape[1], 3) however we need to expand the\n",
    "    # dimension by making the shape (1, inputShape[0], inputShape[1], 3)\n",
    "    # so we can pass it through thenetwork\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "\n",
    "    # pre-process the image using the appropriate function based on the\n",
    "    # model that has been loaded (i.e., mean subtraction, scaling, etc.)\n",
    "    image = preprocess_input(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_model = Xception(weights='imagenet',input_shape=(299, 299, 3))\n",
    "image_model = Model(inputs=base_model.input, outputs=base_model.get_layer('avg_pool').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, LSTM, Flatten, Embedding, Merge\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Word2VecModel(embedding_matrix, num_words, embedding_dim, seq_length, dropout_rate):\n",
    "    print(\"Creating text model...\")\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(num_words, embedding_dim,\n",
    "        weights=[embedding_matrix], input_length=seq_length, trainable=False))\n",
    "    model.add(LSTM(output_dim=512, return_sequences=True, input_shape=(seq_length, embedding_dim)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(LSTM(output_dim=512, return_sequences=False))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1024, activation='tanh'))\n",
    "    return model\n",
    "\n",
    "def img_model(dropout_rate):\n",
    "    print(\"Creating image model...\")\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1024, input_dim=2048, activation='tanh'))\n",
    "    return model\n",
    "\n",
    "def vqa_model(embedding_matrix, num_words, embedding_dim, seq_length, dropout_rate, num_classes):\n",
    "    vgg_model = img_model(dropout_rate)\n",
    "    lstm_model = Word2VecModel(embedding_matrix, num_words, embedding_dim, seq_length, dropout_rate)\n",
    "    print(\"Merging final model...\")\n",
    "    fc_model = Sequential()\n",
    "    fc_model.add(Merge([vgg_model, lstm_model], mode='mul'))\n",
    "    fc_model.add(Dropout(dropout_rate))\n",
    "    fc_model.add(Dense(1000, activation='tanh'))\n",
    "    fc_model.add(Dropout(dropout_rate))\n",
    "    fc_model.add(Dense(num_classes, activation='softmax'))\n",
    "    fc_model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "    return fc_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_question_features(question):\n",
    "    ''' For a given question, a unicode string, returns the time series vector\n",
    "    with each word (token) transformed into a 300 dimension representation\n",
    "    calculated using Glove Vector '''\n",
    "    word_embeddings = spacy.load('en', vectors='en_glove_cc_300_1m_vectors')\n",
    "    tokens = word_embeddings(question)\n",
    "    question_tensor = np.zeros((1, len(tokens), 300))\n",
    "    for j in xrange(len(tokens)):\n",
    "            question_tensor[0,j,:] = tokens[j].vector\n",
    "    return question_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Language not supported: /home/ubuntu/data_processed/data/glove.6B.300d.txt",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-b1c7bba91af4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/ubuntu/data_processed/data/glove.6B.300d.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.5/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mmeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_package_meta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mlang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lang'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmeta\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'lang'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmeta\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_lang_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0moverrides\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'meta'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0moverrides\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'path'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.5/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mget_lang_class\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mlang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[^a-zA-Z0-9]'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlang\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mLANGUAGES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Language not supported: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mLANGUAGES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Language not supported: /home/ubuntu/data_processed/data/glove.6B.300d.txt"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en', vectors='/home/ubuntu/data_processed/data/glove.6B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "obama = nlp(u\"obama\")\n",
    "putin = nlp(u\"putin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "apples = doc7[0]\n",
    "oranges = doc7[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obama.similarity(putin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.5.2 |Anaconda custom (64-bit)| (default, Jul  2 2016, 17:53:06) \n",
      "[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux\n",
      "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
      ">>> \n",
      "KeyboardInterrupt\n",
      ">>> \n",
      ">>> "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open ('/home/ubuntu/data_processed/data/glove.6B.300d.txt', 'r') as f:\n",
    "    txt = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embeddings = {}\n",
    "\n",
    "for word in txt:\n",
    "    word = word.split()\n",
    "    text_word = word[0]\n",
    "    vector_word = np.array([float(val) for val in word[1:]])\n",
    "    embeddings[text_word] = vector_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings['obama'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
